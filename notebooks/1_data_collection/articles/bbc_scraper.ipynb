{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper for the wayback machine BBC news archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import json\n",
    "import calendar\n",
    "import time\n",
    "from newspaper import Article\n",
    "from newspaper import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define wayback machine archive Url's for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all urls for captures by the wayback machine in a specific timeframe\n",
    "def get_urls_of_captures():\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # change following values to modify the request\n",
    "    url_of_interest = \"bbc.com/news\"\n",
    "    from_date = \"20220101\"\n",
    "    to_date = \"20220201\" # include first of next month to be sure to scrape all articles of last day\n",
    "    output_format =\"json\"\n",
    "\n",
    "    # set final url together\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the cache for newspaper3k library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to delete cache of the newspaper3k library (the cache will return items from the previous scrape if not deleted)\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on operating system\n",
    "    # current location is the filepath to the feed_category_cache on windows\n",
    "    feed_category_cache_path = r\"C:\\Users\\steni\\AppData\\Local\\Temp\\.newspaper_scraper\\feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert scraped data to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the urls from the Wayback machine cdx server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "urls_of_captures = get_urls_of_captures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store articles and define the user agent (helps with avoiding timeouts due to too many requests) \n",
    "counter_url = 0\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "scraped_urls = []\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "config.request_timeout = 10\n",
    "\n",
    "\n",
    "for url in urls_of_captures:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # building the news source\n",
    "    bbc = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in bbc.articles:\n",
    "        try:\n",
    "            print(\"article\", counter)\n",
    "            counter += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"bbc.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "\n",
    "            sleep(randint(4, 7))\n",
    "\n",
    "            page = Article(article_url, config=config)\n",
    "            page.download()\n",
    "            article_meta_data = page.meta_data\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(page.html, 'html.parser')\n",
    "            page.parse()\n",
    "            bbc_dictionary = json.loads(\"\".join(soup.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n",
    "            \n",
    "            #find date\n",
    "            article_date = [value for (key, value) in bbc_dictionary.items() if key == 'datePublished']\n",
    "\n",
    "            # find ul list containing tags\n",
    "            parent = soup.find(\"section\").find(\"ul\")\n",
    "            article_keywords = list(parent.descendants)\n",
    "            \n",
    "            # find author\n",
    "            article_authors = [value['name'] for (key, value) in bbc_dictionary.items() if key == 'author']\n",
    "            \n",
    "            # collect title and summary\n",
    "            article_title = [value for (key, value) in bbc_dictionary.items() if key == 'headline']\n",
    "            text = soup.find_all(\"p\", {\"class\": \"ssrcss-1q0x1qg-Paragraph eq5iqo00\"})\n",
    "            article_text = \"\"\n",
    "            for element in text:\n",
    "                article_text += \"\\n\" + \"\".join(element.findAll(string = True)) \n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title or article_title == None:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text or article_text == None:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url or article_url == None:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date:\n",
    "                article_date = article_date\n",
    "            if article_keywords and article_keywords != None:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors and article_authors != None:\n",
    "                article_authors = article_authors\n",
    "            \n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls.append(main_article_url)\n",
    "            sleep(randint(5, 8))\n",
    "        \n",
    "        except Exception as error:\n",
    "            collection_of_failed_scraped_articles.append([article_url, error])\n",
    "            continue\n",
    "    \n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# save data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(failed_articles_df))\n",
    "print(len(scraped_articles_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the scraped dataset.\n",
    "# removes some rows that are scraped from pages that are not actually articles due to the way BBC is structured.  \n",
    "scraped_df = scraped_articles_df\n",
    "d = scraped_df.loc[scraped_df[\"Title\"] == \"N/A\"]\n",
    "d1 = scraped_df.loc[scraped_df[\"text\"] == \"N/A\"]\n",
    "scraped_df.drop(d.index, inplace= True)\n",
    "scraped_df.drop(d1.index, inplace= True)\n",
    "#scraped_articles_df[[\"Title\"]] = scraped_articles_df[[\"Title\"]].str[0]\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_df.to_csv(f'{time_stamp}_articles.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

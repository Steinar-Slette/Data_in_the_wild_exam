{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    " This notebook does the groundwork to be able to start sending requests to the unofficial Pytrends API.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shoutout to all scrapers!\n",
    "<b>Consider changing get request to post request in pytrends library on your local machine.</b>\n",
    "\n",
    "Google is making it difficult for us by changing the terms of service in the last period. The change is that their API returns 429 responses way more often when the user is considered a scraper by the website. Anecdotal evidence from yours truly suggests that 429 errors may be more easily prevented by doing it.\n",
    "\n",
    "We have changed a get request to a post request based on advice from StackOverflow. Credits: https://stackoverflow.com/questions/75744524/pytends-api-throwing-429-error-even-if-the-request-was-made-very-first-time?fbclid=IwAR2j07FLpeXFLQcj1fFiPQyU19xrY0lFr5RuRjYLXT9p8LyQkUGzRxBOcrU\n",
    "\n",
    "If you still get 429 requests and you don't need a huge amount of data (~5000 keywords is still okay), you can try resetting your router to get a new public IP address. If you need more data or it doesn't work for you, you might need to consider using proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we must determine an appropriate pre-defined element\n",
    "The Pytrends library is also just a scraper, and the restriction of compared elements is at 5 on Google Trends. However, the scores are relative. With some (relatively) smart math, we can get results for all the tags. The quality of the data relies on selecting an appropriate pre-defined element that will subsequently be sent in every request to be able to normalize the\n",
    "results and get a relative score for more than 5 elements.\n",
    " \n",
    "\n",
    "The method will combine two approaches:\n",
    "\n",
    "Approach 1 - We send 5 batches, one <b>*arbitrary*</b> element in each batch is pre-defined. We take median for each response (tag).\n",
    "The purpose of this approach is to maintain consistency of relative scores when choosing our pre-defined element. If we only applied this method,\n",
    "we might pick a keyword that is too unpopular or popular, and that would skew the results.\n",
    "\n",
    "Approach 2 - We send 5 batches that contain independent elements\n",
    "We do this because we also want to reduce bias in case our arbitrary pre-defined element is heavily biased.\n",
    "We combine the 10 tags with their median value, and choose a median value from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TrendReq request\n",
    "req = TrendReq(timeout=None, retries=10, backoff_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reshape the data by taking the average of the column for each tag\n",
    "def get_average(api_results):\n",
    "    results_average = []\n",
    "    for analyzed_batch in api_results:\n",
    "            analyzed = pd.DataFrame()\n",
    "            #Skip the first column, as that is the date\n",
    "            for column_name in analyzed_batch.columns:\n",
    "                #As we have time series data, we get the average for the entire month for each column\n",
    "                average_data = analyzed_batch[column_name].mean()\n",
    "                analyzed[column_name] = pd.Series([average_data])\n",
    "                # Convert DataFrame to a dictionary\n",
    "            dict_data = analyzed.to_dict(orient='records')[0]\n",
    "            results_average.append(dict_data)\n",
    "    #Returns an array of dictionaries containing tag and its average\n",
    "    return results_average\n",
    "\n",
    "def get_median_dict(averaged_api_results):\n",
    "    aux = []\n",
    "    output = {}\n",
    "    for dict in averaged_api_results:\n",
    "        median_value = statistics.median(dict.values())\n",
    "        closest_key = min(dict, key=lambda key: abs(dict[key] - median_value))\n",
    "        aux.append({closest_key: dict[closest_key]})\n",
    "    for dict in aux:\n",
    "        output.update(dict)\n",
    "    #Returns a dictionary containing tags and its medians\n",
    "    return output\n",
    "\n",
    "def normalize(averaged_api_results):\n",
    "    aux = []\n",
    "    normalized_dict = {}\n",
    "    aux.append(averaged_api_results[0])\n",
    "    for i in range (1, len(averaged_api_results)):\n",
    "        # We get the pre-defined element from each dictionary (the last element) \n",
    "        # It is possible that we divide by 0 here\n",
    "        # In that case, it is clear that there was not enough collected data for our pre-defined element \n",
    "        # This means that we can just divide each tag value by 1 (having no effect)\n",
    "        value_to_normalize = list(averaged_api_results[i].values())[-1]\n",
    "        print('Value used to normalize:', value_to_normalize)\n",
    "        value_to_normalize = 1 if value_to_normalize == 0 else value_to_normalize\n",
    "        normalization_factor = list(averaged_api_results[0].values())[-1] / value_to_normalize\n",
    "        print('Before:', averaged_api_results[i])\n",
    "        averaged_api_results[i] = {key: value*normalization_factor for key, value in averaged_api_results[i].items()}\n",
    "        print('After:', averaged_api_results[i])\n",
    "        aux.append(averaged_api_results[i])\n",
    "    for dict in aux:\n",
    "        normalized_dict.update(dict)\n",
    "    return normalized_dict\n",
    "\n",
    "def build_csv(normalized_dict):\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\pytrends\\testing\\skynews\\test.csv\"\n",
    "\n",
    "    # Write the dictionary to a CSV file\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['tag_name', 'relative_popularity'])\n",
    "\n",
    "        # Write data\n",
    "        for tag_name, relative_popularity in normalized_dict.items():\n",
    "            csv_writer.writerow([tag_name, relative_popularity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predefined_element(unique_elements, timeframe):\n",
    "    final_dict = {}\n",
    "    arbitrary_response = []\n",
    "    independent_response = []\n",
    "    arbitrary_median = []\n",
    "    independent_median = []\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "\n",
    "    for i in range (1, 20, 4):\n",
    "        batch = unique_elements[i:i+4]\n",
    "        #Add the pre-defined element\n",
    "        batch.append(unique_elements[0])\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        time.sleep(5)\n",
    "        arbitrary_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(arbitrary_response)\n",
    "    arbitrary_median = get_median_dict(averaged_response)\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "\n",
    "    for i in range (0, 25, 5):\n",
    "        batch = unique_elements[i:i+5]\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        time.sleep(5)\n",
    "        response = req.interest_over_time()\n",
    "        independent_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(independent_response)\n",
    "    independent_median = get_median_dict(averaged_response)\n",
    "\n",
    "    # Iterate over the union of keys from both dictionaries\n",
    "    common_keys = arbitrary_median.keys() & independent_median.keys()\n",
    "    \n",
    "    for k in common_keys:\n",
    "        average_of_medians = (arbitrary_median[k] + independent_median[k])/2\n",
    "        final_dict.update({k: average_of_medians})\n",
    "        del arbitrary_median[k], independent_median[k]\n",
    "    \n",
    "    final_dict.update(arbitrary_median)\n",
    "    final_dict.update(independent_median)\n",
    "    \n",
    "    median_value = statistics.median(final_dict.values())\n",
    "    predefined_element = min(final_dict, key=lambda key: abs(final_dict[key] - median_value))\n",
    "\n",
    "    print(f'It seems that {predefined_element} is the most appropriate pre-defined element')\n",
    "    return predefined_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_request(unique_elements, timeframe):\n",
    "    # Batch size is limited to 5 by Google Trends, 1st element is always pre-defined\n",
    "    batch_size = 4\n",
    "    output = []\n",
    "\n",
    "    predefined_element = find_predefined_element(unique_elements, timeframe)\n",
    "    unique_elements.remove(predefined_element)\n",
    "    # Loop through the array in batches\n",
    "    for i in range(0, len(unique_elements), batch_size):\n",
    "        #Extract the batch of 4 elements\n",
    "        batch = unique_elements[i:i+batch_size]\n",
    "        # Add the pre-defined element \n",
    "        batch.append(predefined_element)\n",
    "\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        output.append(response.iloc[:, :-1])\n",
    "        \n",
    "        #Wait a little so I don't overwhelm the API with requests\n",
    "        time.sleep(5)\n",
    "    #Returns an array of data frames, each with 5 tag columns and their score in time series\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most appropriate pre-defined element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "## TODO replace with your path\n",
    "file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\skynews\\articles\\2022-01_skynews_articles.csv\"\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #When csv is loaded, the Tags array is recognized as a string, this casts it to an array\n",
    "    df['Tags'] = df['Tags'].apply(ast.literal_eval)\n",
    "else:\n",
    "    print('The path is invalid')\n",
    "\n",
    "#Get unique tags\n",
    "unique_tags = []\n",
    "for tags in (df['Tags']):\n",
    "    for element in tags:\n",
    "        if element not in unique_tags:\n",
    "            unique_tags.append(element)\n",
    "\n",
    "results = build_request(unique_tags, '2022-01-01 2022-01-31')\n",
    "results = get_average(results)\n",
    "results = normalize(results)\n",
    "build_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrangling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import json\n",
    "import nltk\n",
    "import ssl\n",
    "import calendar\n",
    "import time\n",
    "from newspaper import Article\n",
    "from newspaper import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\steni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual creation get_urls_of_captures list due to the archive api being down \n",
    "def manual_get_urls_of_captures(): \n",
    "    #01.01.2022 - 31.01.2022\n",
    "    final_url_list = [\n",
    "        \"https://web.archive.org/web/20220101101506/http://www.bbc.com/\", \n",
    "        \"https://web.archive.org/web/20220102101510/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220103101535/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220104104015/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220105111551/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220106111553/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220107111707/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220108113014/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220109121526/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220110121539/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220111122739/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220112124750/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220113131043/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220114131546/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220115131717/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220116141608/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220117141525/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220118151542/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220119161414/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220120163833/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220121171555/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220122171711/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220123171737/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220124173041/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220125174040/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220126174110/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220127175131/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220128175546/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220129181124/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220130182141/https://www.bbc.com/\", \n",
    "        \"https://web.archive.org/web/20220131183052/https://www.bbc.com/\",\n",
    "        ]\n",
    "    return final_url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all urls for captures by the wayback machine in a specific timeframe\n",
    "def get_urls_of_captures():\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # change following values to modify the request\n",
    "    url_of_interest = \"bbc.com\"\n",
    "    from_date = \"20220101\"\n",
    "    to_date = \"20220201\" # include first of next month to be sure to scrape all articles of last day\n",
    "    output_format =\"json\"\n",
    "\n",
    "    # set final url together\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to delete cache of the newspaper3k library\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on operating system\n",
    "    # current location is the filepath to the feed_category_cache on windows\n",
    "    feed_category_cache_path = r\"C:\\Users\\steni\\AppData\\Local\\Temp\\.newspaper_scraper\\feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_of_captures = get_urls_of_captures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_of_captures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 0\n",
      "article 0\n",
      "article 1\n",
      "article 2\n",
      "article 3\n",
      "article 4\n",
      "article 5\n",
      "article 6\n",
      "article 7\n",
      "article 8\n",
      "article 9\n",
      "article 10\n",
      "article 11\n",
      "article 12\n",
      "article 13\n",
      "article 14\n",
      "article 15\n",
      "article 16\n",
      "article 17\n",
      "article 18\n",
      "article 19\n",
      "article 20\n",
      "article 21\n",
      "article 22\n",
      "article 23\n",
      "article 24\n",
      "article 25\n",
      "article 26\n",
      "article 27\n",
      "article 28\n",
      "article 29\n",
      "article 30\n",
      "article 31\n",
      "article 32\n",
      "article 33\n",
      "article 34\n",
      "article 35\n",
      "article 36\n",
      "article 37\n",
      "article 38\n",
      "article 39\n",
      "article 40\n",
      "article 41\n",
      "article 42\n",
      "article 43\n",
      "article 44\n",
      "article 45\n",
      "article 46\n",
      "article 47\n",
      "article 48\n",
      "article 49\n",
      "article 50\n",
      "article 51\n",
      "article 52\n",
      "article 53\n",
      "article 54\n",
      "article 55\n",
      "article 56\n",
      "article 57\n",
      "article 58\n",
      "article 59\n",
      "article 60\n",
      "article 61\n",
      "article 62\n",
      "article 63\n",
      "article 64\n",
      "article 65\n",
      "article 66\n",
      "article 67\n",
      "article 68\n",
      "article 69\n",
      "article 70\n",
      "article 71\n",
      "article 72\n",
      "article 73\n",
      "article 74\n",
      "article 75\n",
      "article 76\n",
      "article 77\n",
      "article 78\n",
      "article 79\n",
      "article 80\n",
      "article 81\n",
      "article 82\n",
      "article 83\n",
      "article 84\n",
      "article 85\n",
      "article 86\n",
      "article 87\n",
      "article 88\n",
      "article 89\n"
     ]
    }
   ],
   "source": [
    "counter_url = 0\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "scraped_urls = []\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "config.request_timeout = 10\n",
    "\n",
    "\n",
    "for url in urls_of_captures[0:1]:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # building the news source\n",
    "    bbc = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in bbc.articles:\n",
    "        try:\n",
    "            print(\"article\", counter)\n",
    "            counter += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"bbc.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "\n",
    "            sleep(randint(5, 8))\n",
    "\n",
    "            page = Article(article_url, config=config)\n",
    "            page.download()\n",
    "            article_meta_data = page.meta_data\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(page.html, 'html.parser')\n",
    "            page.parse()\n",
    "            bbc_dictionary = json.loads(\"\".join(soup.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n",
    "            \n",
    "            #find date\n",
    "            article_date = [value for (key, value) in bbc_dictionary.items() if key == 'datePublished']\n",
    "\n",
    "            # find ul list containing tags\n",
    "            parent = soup.find(\"section\").find(\"ul\")\n",
    "            article_keywords = list(parent.descendants)\n",
    "            \n",
    "            # find author\n",
    "            article_authors = [value['name'] for (key, value) in bbc_dictionary.items() if key == 'author']\n",
    "            \n",
    "            # collect title and summary\n",
    "            article_title = [value for (key, value) in bbc_dictionary.items() if key == 'headline']\n",
    "            text = soup.find_all(\"p\", {\"class\": \"ssrcss-1q0x1qg-Paragraph eq5iqo00\"})\n",
    "            article_text = \"\"\n",
    "            for element in text:\n",
    "                article_text += \"\\n\" + \"\".join(element.findAll(string = True)) \n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title or article_title == None:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text or article_text == None:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url or article_url == None:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date:\n",
    "                article_date = article_date\n",
    "            if article_keywords and article_keywords != None:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors and article_authors != None:\n",
    "                article_authors = article_authors\n",
    "            \n",
    "            # use nlp to get keywords of article\n",
    "            page.nlp()\n",
    "            article_nlp_keywords = page.keywords\n",
    "            if not article_nlp_keywords:\n",
    "                article_nlp_keywords = \"N/A\"\n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_nlp_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls.append(main_article_url)\n",
    "            sleep(randint(5, 8))\n",
    "        \n",
    "        except Exception as error:\n",
    "            collection_of_failed_scraped_articles.append([article_url, error])\n",
    "            continue\n",
    "    \n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"NLP Keywords\", \"text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# save data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(failed_articles_df))\n",
    "print(len(scraped_articles_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scraped_articles_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Tags</th>\n",
       "      <th>NLP Keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Covid-19: WHO chief optimistic disease will b...</td>\n",
       "      <td>[2021-12-31T19:28:26.000Z]</td>\n",
       "      <td>[BBC News]</td>\n",
       "      <td>[[[World Health Organization (WHO)]], [World H...</td>\n",
       "      <td>[missed, beaten, disease, dose, optimistic, ra...</td>\n",
       "      <td>\\nThis video can not be played\\nThe World Heal...</td>\n",
       "      <td>https://web.archive.org/web/20220101000057/htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  [Covid-19: WHO chief optimistic disease will b...   \n",
       "\n",
       "                         Date     Authors  \\\n",
       "0  [2021-12-31T19:28:26.000Z]  [BBC News]   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [[[World Health Organization (WHO)]], [World H...   \n",
       "\n",
       "                                        NLP Keywords  \\\n",
       "0  [missed, beaten, disease, dose, optimistic, ra...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nThis video can not be played\\nThe World Heal...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://web.archive.org/web/20220101000057/htt...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import json\n",
    "import nltk\n",
    "import ssl\n",
    "import calendar\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\steni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual creation get_urls_of_captures list due to the archive api being down \n",
    "def manual_get_urls_of_captures(): \n",
    "    #01.01.2022 - 31.01.2022\n",
    "    final_url_list = [\n",
    "        \"https://web.archive.org/web/20220101101506/http://www.bbc.com/\", \n",
    "        \"https://web.archive.org/web/20220102101510/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220103101535/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220104104015/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220105111551/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220106111553/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220107111707/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220108113014/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220109121526/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220110121539/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220111122739/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220112124750/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220113131043/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220114131546/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220115131717/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220116141608/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220117141525/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220118151542/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220119161414/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220120163833/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220121171555/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220122171711/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220123171737/http://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220124173041/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220125174040/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220126174110/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220127175131/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220128175546/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220129181124/https://www.bbc.com/\",\n",
    "        \"https://web.archive.org/web/20220130182141/https://www.bbc.com/\", \n",
    "        \"https://web.archive.org/web/20220131183052/https://www.bbc.com/\",\n",
    "        ]\n",
    "    return final_url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all urls for captures by the wayback machine in a specific timeframe\n",
    "def get_urls_of_captures():\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # change following values to modify the request\n",
    "    url_of_interest = \"bbc.com\"\n",
    "    from_date = \"20220101\"\n",
    "    to_date = \"20220201\" # include first of next month to be sure to scrape all articles of last day\n",
    "    output_format =\"json\"\n",
    "\n",
    "    # set final url together\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to delete cache of the newspaper3k library\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on operating system\n",
    "    # current location is the filepath to the feed_category_cache on windows\n",
    "    feed_category_cache_path = r\"C:\\Users\\steni\\AppData\\Local\\Temp\\.newspaper_scraper\\feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_of_captures = get_urls_of_captures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_of_captures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 0\n",
      "article 0\n",
      "article 1\n",
      "article 2\n",
      "article 3\n",
      "article 4\n",
      "article 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steni\\AppData\\Local\\Temp\\ipykernel_9244\\3884213132.py:44: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  article_text += \"\\n\" + \"\".join(element.findAll(text = True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article 6\n",
      "article 7\n",
      "article 8\n",
      "article 9\n"
     ]
    }
   ],
   "source": [
    "counter_url = 0\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "\n",
    "scraped_urls = []\n",
    "\n",
    "\n",
    "\n",
    "for url in urls_of_captures:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # building the news source\n",
    "    bbc = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in bbc.articles:\n",
    "        try:\n",
    "            print(\"article\", counter)\n",
    "            counter += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"bbc.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "\n",
    "            article.download()\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(article.html, 'html.parser')\n",
    "            article_date = soup.find(\"time\")\n",
    "            parent = soup.find(\"section\").find(\"ul\")\n",
    "            article_keywords = list(parent.descendants)\n",
    "            article_authors = soup.select(\"div.ssrcss-68pt20-Text-TextContributorName.e8mq1e96\")\n",
    "            \n",
    "            # collect title\n",
    "            article.parse()\n",
    "            article_title = article.title\n",
    "            text = soup.find_all(\"p\", {\"class\": \"ssrcss-1q0x1qg-Paragraph eq5iqo00\"})\n",
    "            article_text = \"\"\n",
    "            for element in text:\n",
    "                article_text += \"\\n\" + \"\".join(element.findAll(text = True)) \n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date.has_attr(\"datetime\"):\n",
    "                article_date = article_date[\"datetime\"]\n",
    "            if article_keywords:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors:\n",
    "                article_authors = clean_scraped_date(article_authors)\n",
    "            \n",
    "            # use nlp to get keywords of article\n",
    "            article.nlp()\n",
    "            article_nlp_keywords = article.keywords\n",
    "            if not article_nlp_keywords:\n",
    "                article_nlp_keywords = \"N/A\"\n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_nlp_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls.append(main_article_url)\n",
    "            sleep(randint(4, 7))\n",
    "        \n",
    "        except Exception as error:\n",
    "            collection_of_failed_scraped_articles.append([article_url, error])\n",
    "            continue\n",
    "    \n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"NLP Keywords\", \"text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# save data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(failed_articles_df))\n",
    "print(len(scraped_articles_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Tags</th>\n",
       "      <th>NLP Keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covid-19: WHO chief optimistic disease will be...</td>\n",
       "      <td>2021-12-31T19:28:26.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>[world health organization (who), coronavirus ...</td>\n",
       "      <td>[dose, received, europe, target, end, rates, c...</td>\n",
       "      <td>\\nThis video can not be played\\nThe World Heal...</td>\n",
       "      <td>https://web.archive.org/web/20220101000057/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News</td>\n",
       "      <td>2021-12-31T19:28:26.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>[skip to content, accessibility help]</td>\n",
       "      <td>[standup, video, bbc, englishwhy, comedians, c...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>https://web.archive.org/web/20220101000057/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US &amp; Canada</td>\n",
       "      <td>2021-12-31T21:05:12.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>[skip to content, accessibility help]</td>\n",
       "      <td>[grooming, ghislaine, convicted, does, canada,...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>https://web.archive.org/web/20220101000057/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virginia Giuffre: Prince Andrew accuser seeks ...</td>\n",
       "      <td>2021-12-31T23:54:13.000Z</td>\n",
       "      <td>[]</td>\n",
       "      <td>[prince andrew, duke of york, ghislaine maxwel...</td>\n",
       "      <td>[reasonably, private, andrew, lead, giuffre, a...</td>\n",
       "      <td>\\nLawyers for a US woman who has accused Princ...</td>\n",
       "      <td>https://web.archive.org/web/20220101000057/htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Covid-19: WHO chief optimistic disease will be...   \n",
       "1                                           BBC News   \n",
       "2                                        US & Canada   \n",
       "3  Virginia Giuffre: Prince Andrew accuser seeks ...   \n",
       "\n",
       "                       Date Authors  \\\n",
       "0  2021-12-31T19:28:26.000Z      []   \n",
       "1  2021-12-31T19:28:26.000Z      []   \n",
       "2  2021-12-31T21:05:12.000Z      []   \n",
       "3  2021-12-31T23:54:13.000Z      []   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [world health organization (who), coronavirus ...   \n",
       "1              [skip to content, accessibility help]   \n",
       "2              [skip to content, accessibility help]   \n",
       "3  [prince andrew, duke of york, ghislaine maxwel...   \n",
       "\n",
       "                                        NLP Keywords  \\\n",
       "0  [dose, received, europe, target, end, rates, c...   \n",
       "1  [standup, video, bbc, englishwhy, comedians, c...   \n",
       "2  [grooming, ghislaine, convicted, does, canada,...   \n",
       "3  [reasonably, private, andrew, lead, giuffre, a...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nThis video can not be played\\nThe World Heal...   \n",
       "1                                                N/A   \n",
       "2                                                N/A   \n",
       "3  \\nLawyers for a US woman who has accused Princ...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://web.archive.org/web/20220101000057/htt...  \n",
       "1  https://web.archive.org/web/20220101000057/htt...  \n",
       "2  https://web.archive.org/web/20220101000057/htt...  \n",
       "3  https://web.archive.org/web/20220101000057/htt...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

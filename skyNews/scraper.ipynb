{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import json\n",
    "import nltk\n",
    "import ssl\n",
    "import calendar\n",
    "import time\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:37:08.593 Python[2679:40438] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only needed for nlp\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all urls for captures by the wayback machine in a specific timeframe\n",
    "def get_urls_of_captures():\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # change following values to modify the request\n",
    "    url_of_interest = \"news.sky.com\"\n",
    "    from_date = \"20230101\"\n",
    "    to_date = \"20230201\" # include first of next month to be sure to scrape all articles of last day\n",
    "    output_format =\"json\"\n",
    "\n",
    "    # set final url together\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results of cdx server\n",
    "cdx_urls = get_urls_of_captures()\n",
    "\n",
    "# create data frames\n",
    "cdx_urls_df = pd.DataFrame(cdx_urls, columns=[\"cdx_url\"])\n",
    "\n",
    "# safe data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "cdx_urls_df.to_csv(f'{time_stamp}_cdx_urls_bbc_2023.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to delete cache of the newspaper3k library\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on operating system\n",
    "    feed_category_cache_path = \"/private/var/folders/sh/fjb1r_5j6gxcy4lzfcc4_4zr0000gn/T/.newspaper_scraper/feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_of_captures = get_urls_of_captures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 0\n",
      "article 0\n",
      "article 1\n",
      "article 2\n",
      "article 3\n",
      "article 4\n",
      "article 5\n",
      "article 6\n",
      "article 7\n",
      "article 8\n",
      "article 9\n",
      "article 10\n",
      "article 11\n",
      "article 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m main_article_url \u001b[39min\u001b[39;00m scraped_urls:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m sleep(randint(\u001b[39m5\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m page \u001b[39m=\u001b[39m Article(article_url, config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niclasclassen/Desktop/Data_in_the_wild_exam/skyNews/scraper.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m page\u001b[39m.\u001b[39mdownload()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter_url = 0\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "config.request_timeout = 15\n",
    "\n",
    "\n",
    "scraped_urls = {}\n",
    "\n",
    "\n",
    "for url in urls_of_captures:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # building the news source\n",
    "    sleep(randint(5, 8))\n",
    "    sky_news = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in sky_news.articles:\n",
    "        # try:\n",
    "            print(\"article\", counter)\n",
    "            counter += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"news.sky.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "            \n",
    "            sleep(randint(5, 8))\n",
    "            page = Article(article_url, config=config)\n",
    "            page.download()\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(page.html, 'html.parser')\n",
    "            article_date = soup.find(attrs={\"class\": \"sdc-article-date__date-time\"})\n",
    "            article_keywords = soup.find_all(attrs={\"class\": \"sdc-article-tags__link\"})\n",
    "            article_authors = soup.find_all(attrs={\"class\": \"sdc-article-author__link\"})\n",
    "\n",
    "            # collect title\n",
    "            page.parse()\n",
    "            article_title = page.title\n",
    "            article_text = page.text\n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date:\n",
    "                article_date = article_date.string\n",
    "            if article_keywords:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors:\n",
    "                article_authors = clean_scraped_date(article_authors)\n",
    "            \n",
    "            # use nlp to get keywords of article\n",
    "            page.nlp()\n",
    "            article_nlp_keywords = page.keywords\n",
    "            if not article_nlp_keywords:\n",
    "                article_nlp_keywords = \"N/A\"\n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_nlp_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls[main_article_url] = 1\n",
    "\n",
    "        # except Exception as error:\n",
    "        #     collection_of_failed_scraped_articles.append([article_url, error])\n",
    "        #     continue\n",
    "\n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"NLP Keywords\", \"Text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# safe data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"NLP Keywords\", \"Text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# safe data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import json\n",
    "import nltk\n",
    "import ssl\n",
    "import calendar\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:20:17.751 Python[14058:781899] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only needed for nlp\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all urls for captures by the wayback machine in a specific timeframe\n",
    "def get_urls_of_captures():\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # change following values to modify the request\n",
    "    url_of_interest = \"news.sky.com\"\n",
    "    from_date = \"20220101\"\n",
    "    to_date = \"20220201\" # include first of next month to be sure to scrape all articles of last day\n",
    "    output_format =\"json\"\n",
    "\n",
    "    # set final url together\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to delete cache of the newspaper3k library\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on operating system\n",
    "    feed_category_cache_path = \"/private/var/folders/sh/fjb1r_5j6gxcy4lzfcc4_4zr0000gn/T/.newspaper_scraper/feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_of_captures = get_urls_of_captures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 0\n",
      "article 0\n",
      "article 1\n",
      "url 1\n",
      "article 0\n",
      "article 1\n"
     ]
    }
   ],
   "source": [
    "counter_url = 0\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "\n",
    "scraped_urls = []\n",
    "\n",
    "\n",
    "\n",
    "for url in urls_of_captures:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # building the news source\n",
    "    sky_news = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in sky_news.articles:\n",
    "        try:\n",
    "            print(\"article\", counter)\n",
    "            counter += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"news.sky.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "\n",
    "            article.download()\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(article.html, 'html.parser')\n",
    "            article_date = soup.find(attrs={\"class\": \"sdc-article-date__date-time\"})\n",
    "            article_keywords = soup.find_all(attrs={\"class\": \"sdc-article-tags__link\"})\n",
    "            article_authors = soup.find_all(attrs={\"class\": \"sdc-article-author__link\"})\n",
    "\n",
    "            # collect title\n",
    "            article.parse()\n",
    "            article_title = article.title\n",
    "            article_text = article.text\n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date:\n",
    "                article_date = article_date.string\n",
    "            if article_keywords:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors:\n",
    "                article_authors = clean_scraped_date(article_authors)\n",
    "            \n",
    "            # use nlp to get keywords of article\n",
    "            article.nlp()\n",
    "            article_nlp_keywords = article.keywords\n",
    "            if not article_nlp_keywords:\n",
    "                article_nlp_keywords = \"N/A\"\n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_nlp_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls.append(main_article_url)\n",
    "            sleep(randint(4, 7))\n",
    "\n",
    "        except Exception as error:\n",
    "            collection_of_failed_scraped_articles.append([article_url, error])\n",
    "            continue\n",
    "\n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"NLP Keywords\", \"Text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# safe data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper for the wayback machine sky news archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import os\n",
    "import calendar\n",
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "# Libraries for scraping\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Collect all captures of the sky news website from the wayback machine in a specific time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_of_captures(url_of_interest, from_date, to_date, output_format):\n",
    "    base_url =\"http://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "    # create url to scrape from\n",
    "    url = f\"{base_url}?url={url_of_interest}&collapse=digest&from={from_date}&to={to_date}&output={output_format}\"\n",
    "\n",
    "    # request urls of captures from wayback cdx api\n",
    "    urls_of_captures = rq.get(url).text\n",
    "    parse_urls_of_captures = json.loads(urls_of_captures) \n",
    "\n",
    "    # Extracts timestamp and original columns from urls and compiles a url list\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_urls_of_captures)): # start at 1 to skip headers\n",
    "        orig_url = parse_urls_of_captures[i][2]\n",
    "        timestamp = parse_urls_of_captures[i][1]\n",
    "        waylink = timestamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "\n",
    "    # Compile final url pattern\n",
    "    final_urls_of_captures = []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_urls_of_captures.append(final_url)\n",
    "\n",
    "    return final_urls_of_captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change following values to modify the request\n",
    "url_of_interest = \"news.sky.com\"\n",
    "from_date = \"20230101\"\n",
    "to_date = \"20230201\" # include first of next month to be sure to scrape all articles of last day\n",
    "output_format =\"json\"\n",
    "\n",
    "# Store results of cdx server\n",
    "urls_of_captures = get_urls_of_captures(url_of_interest, from_date, to_date, output_format)\n",
    "\n",
    "# create data frames\n",
    "cdx_urls_df = pd.DataFrame(urls_of_captures, columns=[\"cdx_url\"])\n",
    "\n",
    "# safe data frame as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "cdx_urls_df.to_csv(f'{time_stamp}_cdx_urls_skynews.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to delete cache of the newspaper3k library\n",
    "# the base url 'web.archive.org' stays the same and so the library accesses the cache and returns results \n",
    "# without taking into account the changed subdomain.\n",
    "\n",
    "def delete_feed_category_cache_path():\n",
    "    # the following location of the feed_category_cache might be different depending on your system\n",
    "    feed_category_cache_path = \"/private/var/folders/sh/fjb1r_5j6gxcy4lzfcc4_4zr0000gn/T/.newspaper_scraper/feed_category_cache\"\n",
    "\n",
    "    # delete all files inside the cache\n",
    "    for filename in os.listdir(feed_category_cache_path):\n",
    "        file_path = os.path.join(feed_category_cache_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to format data\n",
    "def clean_scraped_date(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item = item.string.lower()\n",
    "        if item not in cleaned_data:\n",
    "            cleaned_data.append(item)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs of newspaper3k\n",
    "config = Config()\n",
    "config.request_timeout = 10 # increase timeout because some sites need longer to load\n",
    "\n",
    "scraped_urls = {} # keep track of scraped url\n",
    "collection_of_scraped_articles = []\n",
    "collection_of_failed_scraped_articles = []\n",
    "\n",
    "counter_url = 0 # keep track of how many url's already got scraped\n",
    "\n",
    "# urls_of_captures is defined under step 1\n",
    "for url in urls_of_captures:\n",
    "    print(\"url\", counter_url)\n",
    "    counter_url += 1\n",
    "    delete_feed_category_cache_path() # throws error if path doesn't exists\n",
    "\n",
    "    counter_sub_urls = 0\n",
    "    \n",
    "    sleep(randint(5, 8)) # timeout to avoid 429 error\n",
    "    # building the news source\n",
    "    sky_news = newspaper.build(url, memoize_articles=False, language='en')\n",
    "\n",
    "    for article in sky_news.articles:\n",
    "        try:\n",
    "            print(\"article\", counter_sub_urls)\n",
    "            counter_sub_urls += 1\n",
    "            article_url = article.url\n",
    "            \n",
    "            main_article_url = article_url.split(\"news.sky.com\")[1]\n",
    "            if main_article_url in scraped_urls:\n",
    "                continue\n",
    "            \n",
    "            sleep(randint(5, 8)) # timeout to avoid 429 error\n",
    "            page = Article(article_url, config=config)\n",
    "            page.download()\n",
    "\n",
    "            # collect specific data of article\n",
    "            soup = BeautifulSoup(page.html, 'html.parser')\n",
    "            article_date = soup.find(attrs={\"class\": \"sdc-article-date__date-time\"})\n",
    "            article_keywords = soup.find_all(attrs={\"class\": \"sdc-article-tags__link\"})\n",
    "            article_authors = soup.find_all(attrs={\"class\": \"sdc-article-author__link\"})\n",
    "\n",
    "            # collect title\n",
    "            page.parse()\n",
    "            article_title = page.title\n",
    "            article_text = page.text\n",
    "\n",
    "            # clean up scraped data\n",
    "            if not article_title:\n",
    "                article_title = \"N/A\"\n",
    "            if not article_text:\n",
    "                article_text = \"N/A\"\n",
    "            if not article_url:\n",
    "                article_url = \"N/A\"\n",
    "            if article_date:\n",
    "                article_date = article_date.string\n",
    "            if article_keywords:\n",
    "                article_keywords = clean_scraped_date(article_keywords)\n",
    "            if article_authors:\n",
    "                article_authors = clean_scraped_date(article_authors)\n",
    "            \n",
    "            collection_of_scraped_articles.append([article_title, \n",
    "                                                    article_date,\n",
    "                                                    article_authors,\n",
    "                                                    article_keywords,\n",
    "                                                    article_text,\n",
    "                                                    article_url])\n",
    "            scraped_urls[main_article_url] = 1\n",
    "\n",
    "        except Exception as error:\n",
    "            collection_of_failed_scraped_articles.append([article_url, error])\n",
    "            continue\n",
    "\n",
    "# create data frames\n",
    "scraped_articles_df = pd.DataFrame(collection_of_scraped_articles, columns=[\"Title\", \"Date\", \"Authors\", \"Tags\", \"Text\", \"Url\"])\n",
    "failed_articles_df = pd.DataFrame(collection_of_failed_scraped_articles, columns=[\"Url\", \"Error\"])\n",
    "\n",
    "# safe data frames as csv with current timestamp\n",
    "current_GMT = time.gmtime()\n",
    "time_stamp = calendar.timegm(current_GMT)\n",
    "scraped_articles_df.to_csv(f'{time_stamp}_articles.csv', sep=';')\n",
    "failed_articles_df.to_csv(f'{time_stamp}_failed.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    " This notebook does the groundwork to be able to start sending requests to the unofficial Pytrends API.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 1)) (4.9.2)\n",
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: urllib3<2 in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: pandasql in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 4)) (0.7.3)\n",
      "Collecting inflection\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: lxml in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: sqlalchemy in c:\\python39\\lib\\site-packages (from pandasql->-r requirements.txt (line 4)) (2.0.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python39\\lib\\site-packages (from sqlalchemy->pandasql->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from sqlalchemy->pandasql->-r requirements.txt (line 4)) (4.7.1)\n",
      "Installing collected packages: inflection\n",
      "Successfully installed inflection-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shoutout to all scrapers!\n",
    "<b>Consider changing get request to post request in pytrends library on your local machine.</b>\n",
    "\n",
    "Google is making it difficult for us by changing the terms of service in the last period. The change is that their API returns 429 responses way more often when the user is considered a scraper by the website. Anecdotal evidence from yours truly suggests that 429 errors may be more easily prevented by doing it.\n",
    "\n",
    "We have changed a get request to a post request based on advice from StackOverflow. Credits: https://stackoverflow.com/questions/75744524/pytends-api-throwing-429-error-even-if-the-request-was-made-very-first-time?fbclid=IwAR2j07FLpeXFLQcj1fFiPQyU19xrY0lFr5RuRjYLXT9p8LyQkUGzRxBOcrU\n",
    "\n",
    "If you still get 429 requests and you don't need a huge amount of data (~5000 keywords is still okay), you can try resetting your router to get a new public IP address. If you need more data or it doesn't work for you, you might need to consider using proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we must determine an appropriate pre-defined element\n",
    "The Pytrends library is also just a scraper, and the restriction of compared elements is at 5 on Google Trends. However, the scores are relative. With some (relatively) smart math, we can get results for all the tags. The quality of the data relies on selecting an appropriate pre-defined element that will subsequently be sent in every request to be able to normalize the\n",
    "results and get a relative score for more than 5 elements.\n",
    " \n",
    "\n",
    "The method will combine two approaches:\n",
    "\n",
    "Approach 1 - We send 5 batches, one <b>*arbitrary*</b> element in each batch is pre-defined. We take median for each response (tag).\n",
    "The purpose of this approach is to maintain consistency of relative scores when choosing our pre-defined element. If we only applied this method,\n",
    "we might pick a keyword that is too unpopular or popular, and that would skew the results.\n",
    "\n",
    "Approach 2 - We send 5 batches that contain independent elements\n",
    "We do this because we also want to reduce bias in case our arbitrary pre-defined element is heavily biased.\n",
    "We combine the 10 tags with their median value, and choose a median value from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TrendReq request\n",
    "req = TrendReq(timeout=None, retries=10, backoff_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reshape the data by taking the average of the column for each tag\n",
    "def get_average(api_results):\n",
    "    results_average = []\n",
    "    for analyzed_batch in api_results:\n",
    "            analyzed = pd.DataFrame()\n",
    "            #Skip the first column, as that is the date\n",
    "            for column_name in analyzed_batch.columns:\n",
    "                #As we have time series data, we get the average for the entire month for each column\n",
    "                average_data = analyzed_batch[column_name].mean()\n",
    "                analyzed[column_name] = pd.Series([average_data])\n",
    "                # Convert DataFrame to a dictionary\n",
    "            dict_data = analyzed.to_dict(orient='records')[0]\n",
    "            results_average.append(dict_data)\n",
    "    #Returns an array of dictionaries containing tag and its average\n",
    "    return results_average\n",
    "\n",
    "def get_median_dict(averaged_api_results):\n",
    "    aux = []\n",
    "    output = {}\n",
    "    for dict in averaged_api_results:\n",
    "        median_value = statistics.median(dict.values())\n",
    "        closest_key = min(dict, key=lambda key: abs(dict[key] - median_value))\n",
    "        aux.append({closest_key: dict[closest_key]})\n",
    "    for dict in aux:\n",
    "        output.update(dict)\n",
    "    #Returns a dictionary containing tags and its medians\n",
    "    return output\n",
    "\n",
    "def normalize(averaged_api_results):\n",
    "    aux = []\n",
    "    normalized_dict = {}\n",
    "    aux.append(averaged_api_results[0])\n",
    "    for i in range (1, len(averaged_api_results)):\n",
    "        # We get the pre-defined element from each dictionary (the last element) \n",
    "        # It is possible that we divide by 0 here\n",
    "        # In that case, it is clear that there was not enough collected data for our pre-defined element \n",
    "        # This means that we can just divide each tag value by 1 (having no effect)\n",
    "        value_to_normalize = list(averaged_api_results[i].values())[-1]\n",
    "        print('Value used to normalize:', value_to_normalize)\n",
    "        value_to_normalize = 1 if value_to_normalize == 0 else value_to_normalize\n",
    "        normalization_factor = list(averaged_api_results[0].values())[-1] / value_to_normalize\n",
    "        print('Before:', averaged_api_results[i])\n",
    "        averaged_api_results[i] = {key: value*normalization_factor for key, value in averaged_api_results[i].items()}\n",
    "        print('After:', averaged_api_results[i])\n",
    "        aux.append(averaged_api_results[i])\n",
    "    for dict in aux:\n",
    "        normalized_dict.update(dict)\n",
    "    return normalized_dict\n",
    "\n",
    "def build_csv(normalized_dict):\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\pytrends\\testing\\skynews\\test.csv\"\n",
    "\n",
    "    # Write the dictionary to a CSV file\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['tag_name', 'relative_popularity'])\n",
    "\n",
    "        # Write data\n",
    "        for tag_name, relative_popularity in normalized_dict.items():\n",
    "            csv_writer.writerow([tag_name, relative_popularity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predefined_element(unique_elements, timeframe):\n",
    "    final_dict = {}\n",
    "    arbitrary_response = []\n",
    "    independent_response = []\n",
    "    arbitrary_median = []\n",
    "    independent_median = []\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "\n",
    "    for i in range (1, 20, 4):\n",
    "        batch = unique_elements[i:i+4]\n",
    "        #Add the pre-defined element\n",
    "        batch.append(unique_elements[0])\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        time.sleep(5)\n",
    "        arbitrary_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(arbitrary_response)\n",
    "    arbitrary_median = get_median_dict(averaged_response)\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "\n",
    "    for i in range (0, 25, 5):\n",
    "        batch = unique_elements[i:i+5]\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        time.sleep(5)\n",
    "        response = req.interest_over_time()\n",
    "        independent_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(independent_response)\n",
    "    independent_median = get_median_dict(averaged_response)\n",
    "\n",
    "    # Iterate over the union of keys from both dictionaries\n",
    "    common_keys = arbitrary_median.keys() & independent_median.keys()\n",
    "    \n",
    "    for k in common_keys:\n",
    "        average_of_medians = (arbitrary_median[k] + independent_median[k])/2\n",
    "        final_dict.update({k: average_of_medians})\n",
    "        del arbitrary_median[k], independent_median[k]\n",
    "    \n",
    "    final_dict.update(arbitrary_median)\n",
    "    final_dict.update(independent_median)\n",
    "    \n",
    "    median_value = statistics.median(final_dict.values())\n",
    "    predefined_element = min(final_dict, key=lambda key: abs(final_dict[key] - median_value))\n",
    "\n",
    "    print(f'It seems that {predefined_element} is the most appropriate pre-defined element')\n",
    "    return predefined_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_request(unique_elements, timeframe):\n",
    "    # Batch size is limited to 5 by Google Trends, 1st element is always pre-defined\n",
    "    batch_size = 4\n",
    "    output = []\n",
    "\n",
    "    predefined_element = find_predefined_element(unique_elements, timeframe)\n",
    "    unique_elements.remove(predefined_element)\n",
    "    # Loop through the array in batches\n",
    "    for i in range(0, len(unique_elements), batch_size):\n",
    "        #Extract the batch of 4 elements\n",
    "        batch = unique_elements[i:i+batch_size]\n",
    "        # Add the pre-defined element \n",
    "        batch.append(predefined_element)\n",
    "\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        output.append(response.iloc[:, :-1])\n",
    "        \n",
    "        #Wait a little so I don't overwhelm the API with requests\n",
    "        time.sleep(5)\n",
    "    #Returns an array of data frames, each with 5 tag columns and their score in time series\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most appropriate pre-defined element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\School\\\\Semester_1\\\\Data_Wrangling\\\\Data_in_the_wild_exam\\\\data\\\\raw\\\\skynews\\\\articles\\\\1700032053_articles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\google_trends_api\\trends.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Read the csv file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSchool\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSemester_1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_Wrangling\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_in_the_wild_exam\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mskynews\u001b[39m\u001b[39m\\\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1700032053_articles.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(file_path, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_path):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\School\\\\Semester_1\\\\Data_Wrangling\\\\Data_in_the_wild_exam\\\\data\\\\raw\\\\skynews\\\\articles\\\\1700032053_articles.csv'"
     ]
    }
   ],
   "source": [
    "#Read the csv file\n",
    "## TODO replace with your path\n",
    "file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\skynews\\articles\\2022-01_skynews_articles.csv\"\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #When csv is loaded, the Tags array is recognized as a string, this casts it to an array\n",
    "    df['Tags'] = df['Tags'].apply(ast.literal_eval)\n",
    "else:\n",
    "    print('The path is invalid')\n",
    "\n",
    "#Get unique tags\n",
    "unique_tags = []\n",
    "for tags in (df['Tags']):\n",
    "    for element in tags:\n",
    "        if element not in unique_tags:\n",
    "            unique_tags.append(element)\n",
    "\n",
    "results = build_request(unique_tags, '2022-01-01 2022-01-31')\n",
    "results = get_average(results)\n",
    "results = normalize(results)\n",
    "build_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrangling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

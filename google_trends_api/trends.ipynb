{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook receives the tags from the scraped articles and sends API requests to unofficial pytrends API.\n",
    "\n",
    "#### Input -> csv files from the websites; arrays of tags are extracted\n",
    "\n",
    "#### Output -> time series of relative popularity of the tags in January 2022 and 2023 in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 1)) (4.9.2)\n",
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: urllib3<2 in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: pandasql in c:\\python39\\lib\\site-packages (from -r requirements.txt (line 4)) (0.7.3)\n",
      "Collecting inflection\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: lxml in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: sqlalchemy in c:\\python39\\lib\\site-packages (from pandasql->-r requirements.txt (line 4)) (2.0.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python39\\lib\\site-packages (from sqlalchemy->pandasql->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from sqlalchemy->pandasql->-r requirements.txt (line 4)) (4.7.1)\n",
      "Installing collected packages: inflection\n",
      "Successfully installed inflection-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider changing get request to post request in pytrends library\n",
    "Google is making it difficult for us by changing the terms of service in the last period. The change is that their API returns 429 responses way more often when the user is considered a scraper by the website.\n",
    "\n",
    "We have changed a get request to a post request based on advice from StackOverflow. Credits: https://stackoverflow.com/questions/75744524/pytends-api-throwing-429-error-even-if-the-request-was-made-very-first-time?fbclid=IwAR2j07FLpeXFLQcj1fFiPQyU19xrY0lFr5RuRjYLXT9p8LyQkUGzRxBOcrU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import csv\n",
    "import ast\n",
    "import inflection\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "from sqlite3 import connect\n",
    "conn = connect(':memory:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we must determine an appropriate pre-defined element\n",
    "The Pytrends library is also just a scraper, and the restriction of compared elements is at 5 on Google Trends. However, the scores are relative. With some (relatively) smart math, we can get results for all the tags. The quality of the data relies on selecting an appropriate pre-defined element that will subsequently be sent in every request to be able to normalize the\n",
    "results and get a relative score for more than 5 elements.\n",
    " \n",
    "\n",
    "The method will combine two approaches:\n",
    "\n",
    "Approach 1 - We send 5 batches, one <b>*arbitrary*</b> element in each batch is pre-defined. We take median for each response (tag).\n",
    "The purpose of this approach is to maintain consistency of relative scores when choosing our pre-defined element. If we only applied this method,\n",
    "we might pick a keyword that is too unpopular or popular, and that would skew the results.\n",
    "\n",
    "Approach 2 - We send 5 batches that contain independent elements\n",
    "We do this because we also want to reduce bias in case our arbitrary pre-defined element is heavily biased.\n",
    "We combine the 10 tags with their median value, and choose a median value from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TrendReq request\n",
    "req = TrendReq(timeout=None, retries=10, backoff_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary functions\n",
    "# We reshape the data by taking the average of the column for each tag\n",
    "def get_average(api_results):\n",
    "    results_average = []\n",
    "    for analyzed_batch in api_results:\n",
    "            analyzed = pd.DataFrame()\n",
    "            #Skip the first column, as that is the date\n",
    "            for column_name in analyzed_batch.columns:\n",
    "                #As we have time series data, we get the average for the entire month for each column\n",
    "                average_data = analyzed_batch[column_name].mean()\n",
    "                analyzed[column_name] = pd.Series([average_data])\n",
    "                # Convert DataFrame to a dictionary\n",
    "            dict_data = analyzed.to_dict(orient='records')[0]\n",
    "            results_average.append(dict_data)\n",
    "    #Returns an array of dictionaries containing tag and its average\n",
    "    return results_average\n",
    "\n",
    "def get_median_dict(averaged_api_results):\n",
    "    aux = []\n",
    "    output = {}\n",
    "    for dict in averaged_api_results:\n",
    "        median_value = statistics.median(dict.values())\n",
    "        closest_key = min(dict, key=lambda key: abs(dict[key] - median_value))\n",
    "        aux.append({closest_key: dict[closest_key]})\n",
    "    for dict in aux:\n",
    "        output.update(dict)\n",
    "    #Returns a dictionary containing tags and its medians\n",
    "    return output\n",
    "\n",
    "def normalize(averaged_api_results):\n",
    "    aux = []\n",
    "    normalized_dict = {}\n",
    "    aux.append(averaged_api_results[0])\n",
    "    for i in range (1, len(averaged_api_results)):\n",
    "        # We get the pre-defined element from each dictionary (the last element) \n",
    "        # It is possible that we divide by 0 here\n",
    "        # In that case, it is clear that there was not enough collected data for our pre-defined element \n",
    "        # This means that we can just divide each tag value by 1 (having no effect)\n",
    "        value_to_normalize = list(averaged_api_results[i].values())[-1]\n",
    "        print('Value used to normalize:', value_to_normalize)\n",
    "        value_to_normalize = 1 if value_to_normalize == 0 else value_to_normalize\n",
    "        normalization_factor = list(averaged_api_results[0].values())[-1] / value_to_normalize\n",
    "        print('Before:', averaged_api_results[i])\n",
    "        averaged_api_results[i] = {key: value*normalization_factor for key, value in averaged_api_results[i].items()}\n",
    "        print('After:', averaged_api_results[i])\n",
    "        aux.append(averaged_api_results[i])\n",
    "    for dict in aux:\n",
    "        normalized_dict.update(dict)\n",
    "    return normalized_dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def perform_normalization(df):\n",
    "# We get the pre-defined element from each dictionary\n",
    "        grouping = df.groupby(['target'], [])\n",
    "        values_to_normalize = df.loc[:, i+5].copy()\n",
    "        print('Value used to normalize:', values_to_normalize)\n",
    "\n",
    "        # First, cast values in columns 1-5 to numeric\n",
    "        # It is possible that we divide by 0 here\n",
    "        # In that case, it is clear that there was not enough collected data for our pre-defined element\n",
    "        # This means that we can just divide each tag value by 1 (having no effect)\n",
    "            \n",
    "        values_to_normalize.iloc[:, 5] = values_to_normalize.iloc[:, [5]].replace(0, 1)\n",
    "\n",
    "        # Now we will normalize each row in the dataframe for columns with indexes 1-5\n",
    "        print('Before:', df.loc[idx, batch])\n",
    "        # Normalize each element in the row\n",
    "        df.loc[idx, batch].iloc[i+1:i+6] = df.loc[idx, batch].iloc[i+1:i+6].div(values_to_normalize.iloc[1:6])\n",
    "        print('After:', df.loc[idx, batch])\n",
    "\n",
    "        # Append the normalized batch dataframe to the list\n",
    "        normalized_dfs.append(df.copy())\n",
    "    \n",
    "                      \n",
    "def chat_normalize_time_series(csv_file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    \n",
    "    # Get the unique tags from the DataFrame\n",
    "    unique_tags = df.columns[0:].tolist()\n",
    "    \n",
    "    # Specify the batch size\n",
    "    batch_size = 7\n",
    "    \n",
    "    # Initialize the index variable\n",
    "    i = 0\n",
    "    \n",
    "    # List to store normalized dataframes\n",
    "    normalized_dfs = []\n",
    "    \n",
    "    # Iterate over data in batches of 7 columns\n",
    "    while i < len(unique_tags):\n",
    "        batch = unique_tags[i : i + batch_size]\n",
    "        \n",
    "        # If it is the last batch, write the CSV file as output\n",
    "        if i + batch_size >= len(unique_tags):\n",
    "            output_csv_path = f\"normalized_output_{i}.csv\"\n",
    "            # Write the processed batch to a new CSV file\n",
    "            df[batch].to_csv(output_csv_path, sep=';', index=False)\n",
    "            print(f\"CSV file saved: {output_csv_path}\")\n",
    "            \n",
    "            # Exit the loop as it's the last batch\n",
    "            break\n",
    "        \n",
    "        # Else, we will transform the original dataframe by normalizing each row in the batc\n",
    "        # Update the index for the next iteration\n",
    "        i += batch_size\n",
    "    \n",
    "    # Concatenate all the normalized dataframes into a single dataframe\n",
    "    final_normalized_df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the final normalized dataframe to a CSV file\n",
    "    final_normalized_df.to_csv(\"final_normalized_output.csv\", sep=';', index=False)\n",
    "    print(\"Final normalized CSV file saved.\")\n",
    "                    \n",
    "\n",
    "def build_csv(normalized_dict):\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\pytrends\\testing\\skynews\\test.csv\"\n",
    "\n",
    "    # Write the dictionary to a CSV file\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['tag_name', 'relative_popularity'])\n",
    "\n",
    "        # Write data\n",
    "        for tag_name, relative_popularity in normalized_dict.items():\n",
    "            csv_writer.writerow([tag_name, relative_popularity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predefined_element(unique_elements, timeframe):\n",
    "    final_dict = {}\n",
    "    arbitrary_response = []\n",
    "    independent_response = []\n",
    "    arbitrary_median = []\n",
    "    independent_median = []\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "\n",
    "    for i in range (1, 20, 4):\n",
    "        batch = unique_elements[i:i+4]\n",
    "        #Add the pre-defined element\n",
    "        batch.append(unique_elements[0])\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        time.sleep(5)\n",
    "        arbitrary_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(arbitrary_response)\n",
    "    arbitrary_median = get_median_dict(averaged_response)\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "\n",
    "    for i in range (0, 25, 5):\n",
    "        batch = unique_elements[i:i+5]\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        time.sleep(5)\n",
    "        response = req.interest_over_time()\n",
    "        independent_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(independent_response)\n",
    "    independent_median = get_median_dict(averaged_response)\n",
    "\n",
    "    # Iterate over the union of keys from both dictionaries\n",
    "    common_keys = arbitrary_median.keys() & independent_median.keys()\n",
    "\n",
    "    # Display the 10 medians\n",
    "    # df = pd.DataFrame(list(arbitrary_median.items()), columns=['tag_name', 'relative_popularity_arbitrary_median'])\n",
    "    # display(df.reset_index().plot(x='tag_name', y='relative_popularity_arbitrary_median', figsize=(120,10), kind='bar'))\n",
    "    # df = pd.DataFrame(list(independent_median.items()), columns=['tag_name', 'relative_popularity_independent_median'])\n",
    "    # display(df.reset_index().plot(x='tag_name', y='relative_popularity_independent_median', figsize=(120,10), kind='bar'))\n",
    "\n",
    "    # It is possible that the keys in the two dictionaries will overlap\n",
    "    # In that case, calculate the average of the medians for the key\n",
    "    for k in common_keys:\n",
    "        average_of_medians = (arbitrary_median[k] + independent_median[k])/2\n",
    "        final_dict.update({k: average_of_medians})\n",
    "        del arbitrary_median[k], independent_median[k]\n",
    "    \n",
    "    final_dict.update(arbitrary_median)\n",
    "    final_dict.update(independent_median)\n",
    "    \n",
    "    median_value = statistics.median(final_dict.values())\n",
    "    predefined_element = min(final_dict, key=lambda key: abs(final_dict[key] - median_value))\n",
    "\n",
    "    print(f'It seems that {predefined_element} is the most appropriate pre-defined element')\n",
    "    return predefined_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the code for finding the pre-defined element, we can build the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_request(unique_elements, timeframe):\n",
    "    # Batch size is limited to 5 by Google Trends, 1st element is always pre-defined\n",
    "    batch_size = 4\n",
    "    output = []\n",
    "\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "    predefined_element = find_predefined_element(unique_elements, timeframe)\n",
    "    unique_elements.remove(predefined_element)\n",
    "    # Loop through the array in batches\n",
    "    for i in range(0, len(unique_elements), batch_size):\n",
    "        #Extract the batch of 4 elements\n",
    "        batch = unique_elements[i:i+batch_size]\n",
    "        # Add the pre-defined element \n",
    "        batch.append(predefined_element)\n",
    "\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        output.append(response.iloc[:, :-1])\n",
    "        \n",
    "        #Wait a little so I don't overwhelm the API with requests\n",
    "        time.sleep(5)\n",
    "    #Returns an array of data frames, each with 5 tag columns and their score in time series\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that russia is the most appropriate pre-defined element\n",
      "Value used to normalize: 18.419354838709676\n",
      "Before: {'jeffrey epstein': 1.7096774193548387, 'california': 7.645161290322581, 'wales': 55.03225806451613, 'data and forensics': 0.0, 'russia': 18.419354838709676}\n",
      "After: {'jeffrey epstein': 0.24252867069657083, 'california': 1.0845149991525904, 'wales': 7.806677588836789, 'data and forensics': 0.0, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 36.935483870967744\n",
      "Before: {'climate change': 5.580645161290323, 'cop26': 1.1290322580645162, 'snow': 52.16129032258065, 'migrant crossings': 0.0, 'russia': 36.935483870967744}\n",
      "After: {'climate change': 0.3947879983096211, 'cop26': 0.07987040428229328, 'snow': 3.6900126778419495, 'migrant crossings': 0.0, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 36.67741935483871\n",
      "Before: {'omicron': 49.54838709677419, 'arthur labinjo-hughes': 0.3548387096774194, 'vladimir putin': 0.8387096774193549, 'australia bushfires': 0.03225806451612903, 'russia': 36.67741935483871}\n",
      "After: {'omicron': 3.5298323261554168, 'arthur labinjo-hughes': 0.025278747127415097, 'vladimir putin': 0.059749765937526594, 'australia bushfires': 0.0022980679206740994, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 3.193548387096774\n",
      "Before: {'live': 26.387096774193548, 'tony blair': 0.8387096774193549, 'newspapers': 0.3225806451612903, 'weather': 65.2258064516129, 'russia': 3.193548387096774}\n",
      "After: {'live': 21.589442815249267, 'tony blair': 0.686217008797654, 'newspapers': 0.26392961876832843, 'weather': 53.36656891495601, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 14.806451612903226\n",
      "Before: {'ghislaine maxwell': 2.193548387096774, 'covid-19': 5.064516129032258, 'kenya': 3.0, 'betty white': 7.645161290322581, 'russia': 14.806451612903226}\n",
      "After: {'ghislaine maxwell': 0.3870967741935483, 'covid-19': 0.8937381404174572, 'kenya': 0.5294117647058822, 'betty white': 1.349146110056926, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 20.967741935483872\n",
      "Before: {'coronavirus': 54.83870967741935, 'canada': 27.419354838709676, 'prince andrew': 17.096774193548388, 'floods': 0.3870967741935484, 'russia': 20.967741935483872}\n",
      "After: {'coronavirus': 6.833746898263026, 'canada': 3.416873449131513, 'prince andrew': 2.1305210918114144, 'floods': 0.04823821339950372, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 34.516129032258064\n",
      "Before: {'ukraine': 28.483870967741936, 'russia': 34.516129032258064}\n",
      "After: {'ukraine': 2.156255652698221, 'russia': 2.6129032258064515}\n"
     ]
    }
   ],
   "source": [
    "#Read the csv file\n",
    "\n",
    "file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\skynews\\articles\\1700032053_articles.csv\"\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #When csv is loaded, the Tags array is recognized as a string, this casts it to an array\n",
    "    df['Tags'] = df['Tags'].apply(ast.literal_eval)\n",
    "else:\n",
    "    print('The path is invalid')\n",
    "\n",
    "#Get unique tags\n",
    "unique_tags = []\n",
    "for tags in (df['Tags']):\n",
    "    for element in tags:\n",
    "        if element not in unique_tags:\n",
    "            unique_tags.append(element)\n",
    "\n",
    "results = build_request(unique_tags, '2022-01-01 2022-01-31')\n",
    "results = get_average(results)\n",
    "results = normalize(results)\n",
    "build_csv(results)\n",
    "\n",
    "## Display the tags and their popularity on a bar chart\n",
    "#df = pd.DataFrame(list(results.items()), columns=['tag_name', 'relative_popularity'])\n",
    "#display(df.reset_index().plot(x='tag_name', y='relative_popularity', figsize=(120,10), kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved: normalized_output_7700.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\google_trends_api\\trends.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m csv_file_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSchool\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSemester_1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_Wrangling\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_in_the_wild_exam\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mintermediate\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mconcated_results_2022\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mall_tags_2022.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m chat_normalize_time_series(csv_file_path)\n",
      "\u001b[1;32mc:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\google_trends_api\\trends.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Concatenate all the normalized dataframes into a single dataframe\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m final_normalized_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(normalized_dfs, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39m# Save the final normalized dataframe to a CSV file\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/School/Semester_1/Data_Wrangling/Data_in_the_wild_exam/google_trends_api/trends.ipynb#X15sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m final_normalized_df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mfinal_normalized_output.csv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    383\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    384\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    385\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    386\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    387\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    388\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    389\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    390\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[0;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[1;32m--> 443\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    445\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    446\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    502\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "csv_file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\intermediate\\concated_results_2022\\all_tags_2022.csv\"\n",
    "chat_normalize_time_series(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\intermediate\\concated_results_2022\\all_tags_2022.csv\"\n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "unique_tags = df.columns[0:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of array: 7707\n",
      "Duplicates in the list: []\n"
     ]
    }
   ],
   "source": [
    "# Using a set to identify duplicates\n",
    "print('Len of array:', len(unique_tags))\n",
    "unique_set = set()\n",
    "duplicates_set = set(x for x in unique_tags if x in unique_set or unique_set.add(x))\n",
    "\n",
    "# Converting the set of duplicates back to a list\n",
    "duplicates_list = list(duplicates_set)\n",
    "\n",
    "print(\"Duplicates in the list:\", duplicates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrangling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

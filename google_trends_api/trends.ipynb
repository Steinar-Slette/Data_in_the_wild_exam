{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook receives the tags from the scraped articles and sends API requests to unofficial pytrends API.\n",
    "\n",
    "#### Input -> csv files from the websites; arrays of tags are extracted\n",
    "\n",
    "#### Output -> time series of relative popularity of the tags in January 2022 and 2023 in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrends\n",
      "  Using cached pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Collecting urllib3<2\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: lxml in c:\\python39\\lib\\site-packages (from pytrends->-r requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\python39\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (1.26.1)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Installing collected packages: urllib3, tzdata, pytz, pandas, pytrends\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "Successfully installed pandas-2.1.3 pytrends-4.9.2 pytz-2023.3.post1 tzdata-2023.3 urllib3-1.26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider changing get request to post request in pytrends library\n",
    "Google is making it difficult for us by changing the terms of service in the last period. The change is that their API returns 429 responses way more often when the user is considered a scraper by the website.\n",
    "\n",
    "We have changed a get request to a post request based on advice from StackOverflow. Credits: https://stackoverflow.com/questions/75744524/pytends-api-throwing-429-error-even-if-the-request-was-made-very-first-time?fbclid=IwAR2j07FLpeXFLQcj1fFiPQyU19xrY0lFr5RuRjYLXT9p8LyQkUGzRxBOcrU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we must determine an appropriate pre-defined element\n",
    "The Pytrends library is also just a scraper, and the restriction of compared elements is at 5 on Google Trends. However, the scores are relative. With some (relatively) smart math, we can get results for all the tags. The quality of the data relies on selecting an appropriate pre-defined element that will subsequently be sent in every request to be able to normalize the\n",
    "results and get a relative score for more than 5 elements.\n",
    " \n",
    "\n",
    "The method will combine two approaches:\n",
    "\n",
    "Approach 1 - We send 5 batches, one <b>*arbitrary*</b> element in each batch is pre-defined. We take median for each response (tag).\n",
    "The purpose of this approach is to maintain consistency of relative scores when choosing our pre-defined element. If we only applied this method,\n",
    "we might pick a keyword that is too unpopular or popular, and that would skew the results.\n",
    "\n",
    "Approach 2 - We send 5 batches that contain independent elements\n",
    "We do this because we also want to reduce bias in case our arbitrary pre-defined element is heavily biased.\n",
    "We combine the 10 tags with their median value, and choose a median value from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TrendReq request\n",
    "req = TrendReq(timeout=None, retries=10, backoff_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary functions\n",
    "# We reshape the data by taking the average of the column for each tag\n",
    "def get_average(api_results):\n",
    "    results_average = []\n",
    "    for analyzed_batch in api_results:\n",
    "            analyzed = pd.DataFrame()\n",
    "            #Skip the first column, as that is the date\n",
    "            for column_name in analyzed_batch.columns:\n",
    "                #As we have time series data, we get the average for the entire month for each column\n",
    "                average_data = analyzed_batch[column_name].mean()\n",
    "                analyzed[column_name] = pd.Series([average_data])\n",
    "                # Convert DataFrame to a dictionary\n",
    "            dict_data = analyzed.to_dict(orient='records')[0]\n",
    "            results_average.append(dict_data)\n",
    "    #Returns an array of dictionaries containing tag and its average\n",
    "    return results_average\n",
    "\n",
    "def get_median_dict(averaged_api_results):\n",
    "    aux = []\n",
    "    output = {}\n",
    "    for dict in averaged_api_results:\n",
    "        median_value = statistics.median(dict.values())\n",
    "        closest_key = min(dict, key=lambda key: abs(dict[key] - median_value))\n",
    "        aux.append({closest_key: dict[closest_key]})\n",
    "    for dict in aux:\n",
    "        output.update(dict)\n",
    "    #Returns a dictionary containing tags and its medians\n",
    "    return output\n",
    "\n",
    "def normalize(averaged_api_results):\n",
    "    aux = []\n",
    "    normalized_dict = {}\n",
    "    aux.append(averaged_api_results[0])\n",
    "    for i in range (1, len(averaged_api_results)):\n",
    "        # We get the pre-defined element from each dictionary (the last element) \n",
    "        # It is possible that we divide by 0 here\n",
    "        # In that case, it is clear that there was not enough collected data for our pre-defined element \n",
    "        # This means that we can just divide each tag value by 1 (having no effect)\n",
    "        value_to_normalize = list(averaged_api_results[i].values())[-1]\n",
    "        print('Value used to normalize:', value_to_normalize)\n",
    "        value_to_normalize = 1 if value_to_normalize == 0 else value_to_normalize\n",
    "        normalization_factor = list(averaged_api_results[0].values())[-1] / value_to_normalize\n",
    "        print('Before:', averaged_api_results[i])\n",
    "        averaged_api_results[i] = {key: value*normalization_factor for key, value in averaged_api_results[i].items()}\n",
    "        print('After:', averaged_api_results[i])\n",
    "        aux.append(averaged_api_results[i])\n",
    "    for dict in aux:\n",
    "        normalized_dict.update(dict)\n",
    "    return normalized_dict\n",
    "\n",
    "def build_csv(normalized_dict):\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\pytrends\\testing\\skynews\\test.csv\"\n",
    "\n",
    "    # Write the dictionary to a CSV file\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['tag_name', 'relative_popularity'])\n",
    "\n",
    "        # Write data\n",
    "        for tag_name, relative_popularity in normalized_dict.items():\n",
    "            csv_writer.writerow([tag_name, relative_popularity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predefined_element(unique_elements, timeframe):\n",
    "    final_dict = {}\n",
    "    arbitrary_response = []\n",
    "    independent_response = []\n",
    "    arbitrary_median = []\n",
    "    independent_median = []\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "\n",
    "    for i in range (1, 20, 4):\n",
    "        batch = unique_elements[i:i+4]\n",
    "        #Add the pre-defined element\n",
    "        batch.append(unique_elements[0])\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        time.sleep(5)\n",
    "        arbitrary_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(arbitrary_response)\n",
    "    arbitrary_median = get_median_dict(averaged_response)\n",
    "\n",
    "    #Shuffle the array to send random elements\n",
    "    random.shuffle(unique_elements)\n",
    "\n",
    "    for i in range (0, 25, 5):\n",
    "        batch = unique_elements[i:i+5]\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        time.sleep(5)\n",
    "        response = req.interest_over_time()\n",
    "        independent_response.append(response.iloc[:, :-1])\n",
    "    \n",
    "    averaged_response = get_average(independent_response)\n",
    "    independent_median = get_median_dict(averaged_response)\n",
    "\n",
    "    # Iterate over the union of keys from both dictionaries\n",
    "    common_keys = arbitrary_median.keys() & independent_median.keys()\n",
    "\n",
    "    # Display the 10 medians\n",
    "    # df = pd.DataFrame(list(arbitrary_median.items()), columns=['tag_name', 'relative_popularity_arbitrary_median'])\n",
    "    # display(df.reset_index().plot(x='tag_name', y='relative_popularity_arbitrary_median', figsize=(120,10), kind='bar'))\n",
    "    # df = pd.DataFrame(list(independent_median.items()), columns=['tag_name', 'relative_popularity_independent_median'])\n",
    "    # display(df.reset_index().plot(x='tag_name', y='relative_popularity_independent_median', figsize=(120,10), kind='bar'))\n",
    "\n",
    "    # It is possible that the keys in the two dictionaries will overlap\n",
    "    # In that case, calculate the average of the medians for the key\n",
    "    for k in common_keys:\n",
    "        average_of_medians = (arbitrary_median[k] + independent_median[k])/2\n",
    "        final_dict.update({k: average_of_medians})\n",
    "        del arbitrary_median[k], independent_median[k]\n",
    "    \n",
    "    final_dict.update(arbitrary_median)\n",
    "    final_dict.update(independent_median)\n",
    "    \n",
    "    median_value = statistics.median(final_dict.values())\n",
    "    predefined_element = min(final_dict, key=lambda key: abs(final_dict[key] - median_value))\n",
    "\n",
    "    print(f'It seems that {predefined_element} is the most appropriate pre-defined element')\n",
    "    return predefined_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the code for finding the pre-defined element, we can build the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_request(unique_elements, timeframe):\n",
    "    # Batch size is limited to 5 by Google Trends, 1st element is always pre-defined\n",
    "    batch_size = 4\n",
    "    output = []\n",
    "\n",
    "    ## Just take the first 30 elements for testing\n",
    "    unique_elements = unique_elements[0:30]\n",
    "    predefined_element = find_predefined_element(unique_elements, timeframe)\n",
    "    unique_elements.remove(predefined_element)\n",
    "    # Loop through the array in batches\n",
    "    for i in range(0, len(unique_elements), batch_size):\n",
    "        #Extract the batch of 4 elements\n",
    "        batch = unique_elements[i:i+batch_size]\n",
    "        # Add the pre-defined element \n",
    "        batch.append(predefined_element)\n",
    "\n",
    "        req.build_payload(batch, geo='GB', timeframe=timeframe)\n",
    "        ##Drop last column from the response\n",
    "        #Last column is an unnecessary boolean\n",
    "        response = req.interest_over_time()\n",
    "        output.append(response.iloc[:, :-1])\n",
    "        \n",
    "        #Wait a little so I don't overwhelm the API with requests\n",
    "        time.sleep(5)\n",
    "    #Returns an array of data frames, each with 5 tag columns and their score in time series\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that russia is the most appropriate pre-defined element\n",
      "Value used to normalize: 18.419354838709676\n",
      "Before: {'jeffrey epstein': 1.7096774193548387, 'california': 7.645161290322581, 'wales': 55.03225806451613, 'data and forensics': 0.0, 'russia': 18.419354838709676}\n",
      "After: {'jeffrey epstein': 0.24252867069657083, 'california': 1.0845149991525904, 'wales': 7.806677588836789, 'data and forensics': 0.0, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 36.935483870967744\n",
      "Before: {'climate change': 5.580645161290323, 'cop26': 1.1290322580645162, 'snow': 52.16129032258065, 'migrant crossings': 0.0, 'russia': 36.935483870967744}\n",
      "After: {'climate change': 0.3947879983096211, 'cop26': 0.07987040428229328, 'snow': 3.6900126778419495, 'migrant crossings': 0.0, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 36.67741935483871\n",
      "Before: {'omicron': 49.54838709677419, 'arthur labinjo-hughes': 0.3548387096774194, 'vladimir putin': 0.8387096774193549, 'australia bushfires': 0.03225806451612903, 'russia': 36.67741935483871}\n",
      "After: {'omicron': 3.5298323261554168, 'arthur labinjo-hughes': 0.025278747127415097, 'vladimir putin': 0.059749765937526594, 'australia bushfires': 0.0022980679206740994, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 3.193548387096774\n",
      "Before: {'live': 26.387096774193548, 'tony blair': 0.8387096774193549, 'newspapers': 0.3225806451612903, 'weather': 65.2258064516129, 'russia': 3.193548387096774}\n",
      "After: {'live': 21.589442815249267, 'tony blair': 0.686217008797654, 'newspapers': 0.26392961876832843, 'weather': 53.36656891495601, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 14.806451612903226\n",
      "Before: {'ghislaine maxwell': 2.193548387096774, 'covid-19': 5.064516129032258, 'kenya': 3.0, 'betty white': 7.645161290322581, 'russia': 14.806451612903226}\n",
      "After: {'ghislaine maxwell': 0.3870967741935483, 'covid-19': 0.8937381404174572, 'kenya': 0.5294117647058822, 'betty white': 1.349146110056926, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 20.967741935483872\n",
      "Before: {'coronavirus': 54.83870967741935, 'canada': 27.419354838709676, 'prince andrew': 17.096774193548388, 'floods': 0.3870967741935484, 'russia': 20.967741935483872}\n",
      "After: {'coronavirus': 6.833746898263026, 'canada': 3.416873449131513, 'prince andrew': 2.1305210918114144, 'floods': 0.04823821339950372, 'russia': 2.6129032258064515}\n",
      "Value used to normalize: 34.516129032258064\n",
      "Before: {'ukraine': 28.483870967741936, 'russia': 34.516129032258064}\n",
      "After: {'ukraine': 2.156255652698221, 'russia': 2.6129032258064515}\n"
     ]
    }
   ],
   "source": [
    "#Read the csv file\n",
    "\n",
    "file_path = r\"C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\skynews\\articles\\1700032053_articles.csv\"\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #When csv is loaded, the Tags array is recognized as a string, this casts it to an array\n",
    "    df['Tags'] = df['Tags'].apply(ast.literal_eval)\n",
    "else:\n",
    "    print('The path is invalid')\n",
    "\n",
    "#Get unique tags\n",
    "unique_tags = []\n",
    "for tags in (df['Tags']):\n",
    "    for element in tags:\n",
    "        if element not in unique_tags:\n",
    "            unique_tags.append(element)\n",
    "\n",
    "results = build_request(unique_tags, '2022-01-01 2022-01-31')\n",
    "results = get_average(results)\n",
    "results = normalize(results)\n",
    "build_csv(results)\n",
    "\n",
    "## Display the tags and their popularity on a bar chart\n",
    "#df = pd.DataFrame(list(results.items()), columns=['tag_name', 'relative_popularity'])\n",
    "#display(df.reset_index().plot(x='tag_name', y='relative_popularity', figsize=(120,10), kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy CSV file created and saved to: C:\\School\\Semester_1\\Data_Wrangling\\Data_in_the_wild_exam\\data\\raw\\pytrends\\testing\\skynews\\test.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrangling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
